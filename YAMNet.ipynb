{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmYonWriqz_t",
        "outputId": "7133a56b-74c6-4daf-b263-2efee3628a90"
      },
      "source": [
        "!tar -xvzf yamnet-tensorflow2-yamnet-v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xzTlOkK_sYD5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa  # For easy audio loading\n",
        "from IPython.display import Audio\n",
        "\n",
        "# Verify the model folder path (usually it extracts to the current dir or a subfolder)\n",
        "model_path = './' # Change if it extracted to a subfolder like 'yamnet_v1'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avFCjTXatJ2E",
        "outputId": "2d754f53-a78e-44c4-c557-8b926ce3e0b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded. It can recognize 521 sounds.\n",
            "Class 42: Cough\n",
            "Class 79: Hiss\n"
          ]
        }
      ],
      "source": [
        "# Load the model into memory\n",
        "model = hub.load(model_path)\n",
        "\n",
        "# Load the class names from your uploaded CSV\n",
        "class_map_path = \"assets/yamnet_class_map.csv\"\n",
        "class_names = pd.read_csv(class_map_path)['display_name'].tolist()\n",
        "\n",
        "print(f\"Model loaded. It can recognize {len(class_names)} sounds.\")\n",
        "# For Sentiguard, check these specifically:\n",
        "print(f\"Class 42: {class_names[42]}\") # Cough\n",
        "print(f\"Class 79: {class_names[79]}\") # Hiss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nAbfOoq8tRzh"
      },
      "outputs": [],
      "source": [
        "def classify_audio(audio_file, top_k=5):\n",
        "    # Load and resample to 16kHz\n",
        "    waveform, sr = librosa.load(audio_file, sr=16000, mono=True)\n",
        "\n",
        "    # Run the model\n",
        "    # YAMNet model expects a 1D float32 waveform at 16kHz\n",
        "    scores, embeddings, spectrogram = model(tf.constant(waveform, dtype=tf.float32))\n",
        "\n",
        "    # Get the top predictions\n",
        "    mean_scores = np.mean(scores, axis=0)\n",
        "    top_k_indices = np.argsort(mean_scores)[::-1][:top_k] # Get indices of top k scores\n",
        "    top_k_predictions = [class_names[i] for i in top_k_indices]\n",
        "    top_k_scores = [mean_scores[i] for i in top_k_indices]\n",
        "\n",
        "    return top_k_predictions, top_k_scores\n",
        "\n",
        "# New test cell - please uncomment and replace 'your_audio_file.wav' with your uploaded file names.\n",
        "# For example:\n",
        "# top_predictions, top_scores = classify_audio('test_hiss.wav')\n",
        "# print(f\"Detected top {len(top_predictions)} predictions:\")\n",
        "# for i in range(len(top_predictions)):\n",
        "#     print(f\"- {top_predictions[i]} ({top_scores[i]*100:.2f}%) \")\n",
        "\n",
        "# top_predictions_2, top_scores_2 = classify_audio('test_cough.wav')\n",
        "# print(f\"\\nDetected top {len(top_predictions_2)} predictions for another file:\")\n",
        "# for i in range(len(top_predictions_2)):\n",
        "#     print(f\"- {top_predictions_2[i]} ({top_scores_2[i]*100:.2f}%) \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "c7Yqa33qtWUB"
      },
      "outputs": [],
      "source": [
        "def sentiguard_monitor(audio_file):\n",
        "    top_predictions, top_scores = classify_audio(audio_file)\n",
        "\n",
        "    print(f\"Detected top sounds: {top_predictions}\")\n",
        "\n",
        "    # Logic for your 'Active Guardian' Pillar\n",
        "    alert_triggered = False\n",
        "    for sound in top_predictions:\n",
        "        if sound in [\"Cough\", \"Hiss\", \"Screaming\"]:\n",
        "            print(f\"⚠️ ALERT: {sound} detected! Triggering Vibration & Logging Evidence...\")\n",
        "            # In the Android app, this is where Pillar 3 (Logging) would start.\n",
        "            alert_triggered = True\n",
        "            break # Trigger alert for the first matching sound\n",
        "\n",
        "    if not alert_triggered:\n",
        "        print(\"Safe environment.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0301e5c"
      },
      "source": [
        "### Classify your own audio file\n",
        "\n",
        "To use this, upload your audio file to Colab (e.g., drag and drop it into the files tab on the left sidebar). Then, replace `'your_audio_file.wav'` in the code below with the name of your uploaded file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9220f68",
        "outputId": "1e10510a-0cd9-4606-a3c9-031dc3dcfc03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Classification results for 'audio_file.mp3':\n",
            "- Cough (43.66%) \n",
            "- Throat clearing (25.39%) \n",
            "- Silence (23.54%) \n",
            "- Speech (11.58%) \n",
            "- Beatboxing (5.13%) \n"
          ]
        }
      ],
      "source": [
        "# Replace 'your_audio_file.wav' with the actual name of your audio file\n",
        "# For example, if you uploaded 'my_test_sound.mp3', change it to classify_audio('my_test_sound.mp3')\n",
        "\n",
        "user_audio_file = 'audio_file.mp3' # <--- Change this line\n",
        "\n",
        "try:\n",
        "    top_predictions, top_scores = classify_audio(user_audio_file)\n",
        "    print(f\"\\nClassification results for '{user_audio_file}':\")\n",
        "    for i in range(len(top_predictions)):\n",
        "        print(f\"- {top_predictions[i]} ({top_scores[i]*100:.2f}%) \")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{user_audio_file}' was not found. Please make sure you have uploaded the file and the filename is correct.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during classification: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "310f5c89",
        "outputId": "76243554-a46c-4d42-9ba1-70dc623909c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (0.25.1)\n"
          ]
        }
      ],
      "source": [
        "# Install pydub for more robust audio file handling, especially for .mp3\n",
        "!pip install pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAQYceJ0vsBx",
        "outputId": "f08f39fa-9739-4736-fe16-3728ffb2002a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conversion complete: sound_classifier.tflite is ready.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 1. Path to your extracted SavedModel directory\n",
        "# Ensure you have run !tar -xvzf yamnet-tensorflow2-yamnet-v1.tar.gz first\n",
        "saved_model_dir = './'\n",
        "\n",
        "# 2. Convert to TFLite\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "\n",
        "# Optional: Add quantization to shrink the size for better Edge performance\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# 3. Save as your preferred filename\n",
        "with open('sound_classifier.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(\"Conversion complete: sound_classifier.tflite is ready.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
